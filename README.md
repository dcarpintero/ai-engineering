# Generative-AI-101

Annotated Notebooks to dive into foundational concepts and state-of-the-art techniques for LLMs and Diffusion models. This is a work in progress, more content will be released on a regular basis.

[![GitHub license](https://img.shields.io/github/license/dcarpintero/generative-ai-101.svg)](https://github.com/dcarpintero/generative-ai-101/blob/master/LICENSE)
[![GitHub contributors](https://img.shields.io/github/contributors/dcarpintero/generative-ai-101.svg)](https://GitHub.com/dcarpintero/generative-ai-101/graphs/contributors/)
[![GitHub issues](https://img.shields.io/github/issues/dcarpintero/generative-ai-101.svg)](https://GitHub.com/dcarpintero/generative-ai-101/issues/)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/dcarpintero/generative-ai-101.svg)](https://GitHub.com/dcarpintero/generative-ai-101/pulls/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

[![GitHub watchers](https://img.shields.io/github/watchers/dcarpintero/generative-ai-101.svg?style=social&label=Watch)](https://GitHub.com/dcarpintero/generative-ai-101/watchers/)
[![GitHub forks](https://img.shields.io/github/forks/dcarpintero/generative-ai-101.svg?style=social&label=Fork)](https://GitHub.com/dcarpintero/generative-ai-101/network/)
[![GitHub stars](https://img.shields.io/github/stars/dcarpintero/generative-ai-101.svg?style=social&label=Star)](https://GitHub.com/dcarpintero/generative-ai-101/stargazers/)

- [00. Transformers Self-Attention Mechanism]()
- [01. In-Context Learning]()
- [02. LLM-Augmentation]()
- [03. Retrieval Augmented Generation]()
- [04. Knowledge Graphs]()
- [05. Fine-Tuning BERT]()
- [06. Fine-Tuning ResNet]()
- [07. Model Optimization: Quantization]()

## 00. Transformers Self-Attention Mechanism

The Transformer architecture, introduced in 2017 by Google and University of Toronto researchers, revolutionized Natural Language Processing (NLP) with its innovative self-attention mechanism. This approach, which replaced traditional Recurrent Neural Networks (RNNs), allows models to learn contextual relationships between words regardless of their position in a sequence. By using attention weights to determine word relevance, Transformers have significantly improved training efficiency and inference accuracy in various NLP tasks such as machine translation and text summarization.

In this notebook, we'll explore the patterns that are typically learned by this self-attention mechanism in multi-heads using bertviz, an interactive tool for visualizing attention in Transformer models:

<p align="center">
  <img src="./static/self_attention_s1.png" width="400">
</p>
<p align="center">Self-Attention Visualization in the BERT model</p>

`Transfomers` `Self-Attention` `BERT` `BertViz`

## 01. In-Content Learning

In Progress

## 02. LLM-Augmentation

In Progress

## 03. Retrieval Augmented Generation

In Progress

## 04. Knowledge Graphs

In Progress

## 05. Fine-Tuning BERT

In Progress

## 06. Fine-Tuning ResNet

In Progress

## 07. Model Optimization: Quantization




